{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Basics\n",
    "\n",
    "Welcome to the section on deep learning! We'll be using Keras with a TensorFlow backend to perform our deep learning operations.\n",
    "\n",
    "This means we should get familiar with some Keras fundamentals and basics!\n",
    "\n",
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the Bank Authentication Data Set to start off with. This data set consists of various image features derived from images that had 400 x 400 pixels. You should note **the data itself that we will be using ARE NOT ACTUAL IMAGES**, they are **features** of images. In the next lecture we will cover grabbing and working with image data with Keras. This notebook focuses on learning the basics of building a neural network with Keras.\n",
    "\n",
    "_____\n",
    "More info on the data set:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. variance of Wavelet Transformed image (continuous) \n",
    "2. skewness of Wavelet Transformed image (continuous) \n",
    "3. curtosis of Wavelet Transformed image (continuous) \n",
    "4. entropy of image (continuous) \n",
    "5. class (integer) \n",
    "\n",
    "## Reading in the Data Set\n",
    "\n",
    "We've already downloaded the dataset, its in the DATA folder. So let's open it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "data = genfromtxt('/Users/Sohaib/Documents/ATOM CAMP/Deep learning/Practice Assignment/bank_note_data.txt', delimiter=',')\n",
    "#delimiter added beacuse this is preloaded data so it needs such features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699,   0.     ],\n",
       "       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ,   0.     ],\n",
       "       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645,   0.     ],\n",
       "       ...,\n",
       "       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ,   1.     ],\n",
       "       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ,   1.     ],\n",
       "       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ,   1.     ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "#0 and 1 shows authentic and false notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699],\n",
       "       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ],\n",
       "       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645],\n",
       "       ...,\n",
       "       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ],\n",
       "       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ],\n",
       "       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Test\n",
    "\n",
    "Its time to split the data into a train/test set. Keep in mind, sometimes people like to split 3 ways, train/test/validation. We'll keep things simple for now. **Remember to check out the video explanation as to why we split and what all the parameters mean!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8734  , -0.033118, -0.20165 ,  0.55774 ],\n",
       "       [ 2.0177  ,  1.7982  , -2.9581  ,  0.2099  ],\n",
       "       [-0.36038 ,  4.1158  ,  3.1143  , -0.37199 ],\n",
       "       ...,\n",
       "       [-7.0364  ,  9.2931  ,  0.16594 , -4.5396  ],\n",
       "       [-3.4605  ,  2.6901  ,  0.16165 , -1.0224  ],\n",
       "       [-3.3582  , -7.2404  , 11.4419  , -0.57113 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "#show length of x_train and x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5691  ,  6.3465  , -0.1828  , -2.4099  ],\n",
       "       [-0.27802 ,  8.1881  , -3.1338  , -2.5276  ],\n",
       "       [ 0.051979,  7.0521  , -2.0541  , -3.1508  ],\n",
       "       ...,\n",
       "       [ 3.5127  ,  2.9073  ,  1.0579  ,  0.40774 ],\n",
       "       [ 5.504   , 10.3671  , -4.413   , -4.0211  ],\n",
       "       [-0.2062  ,  9.2207  , -3.7044  , -6.8103  ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the Data\n",
    "\n",
    "Usually when using Neural Networks, you will get better performance when you standardize the data. Standardization just means normalizing the values to all fit between a certain range, like 0-1, or -1 to 1.\n",
    "\n",
    "The scikit learn library also provides a nice function for this.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_object.fit(X_train)\n",
    "#just finds out the min and the max for the data that you want to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler_object.transform(X_train)\n",
    "#this operation gives us scaler transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test = scaler_object.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have the data scaled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.9274"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8734  , -0.033118, -0.20165 ,  0.55774 ],\n",
       "       [ 2.0177  ,  1.7982  , -2.9581  ,  0.2099  ],\n",
       "       [-0.36038 ,  4.1158  ,  3.1143  , -0.37199 ],\n",
       "       ...,\n",
       "       [-7.0364  ,  9.2931  ,  0.16594 , -4.5396  ],\n",
       "       [-3.4605  ,  2.6901  ,  0.16165 , -1.0224  ],\n",
       "       [-3.3582  , -7.2404  , 11.4419  , -0.57113 ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.44850688e-01, 5.14130449e-01, 2.18194638e-01, 8.50172258e-01],\n",
       "       [6.53339968e-01, 5.82655745e-01, 9.93242398e-02, 8.17696322e-01],\n",
       "       [4.81846700e-01, 6.69377018e-01, 3.61193167e-01, 7.63368407e-01],\n",
       "       ...,\n",
       "       [4.11050776e-04, 8.63104170e-01, 2.34046756e-01, 3.74261253e-01],\n",
       "       [2.58284115e-01, 6.16029366e-01, 2.33861752e-01, 7.02643151e-01],\n",
       "       [2.65661395e-01, 2.44444278e-01, 7.20316361e-01, 7.44775785e-01]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network with Keras\n",
    "\n",
    "Let's build a simple neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sohaib\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model\n",
    "model = Sequential()\n",
    "# 8 Neurons, expects input of 4 features. \n",
    "# Play around with the number of neurons!!\n",
    "model.add(Dense(4, input_dim=4, activation='relu'))\n",
    "# Add another Densely Connected layer (every neuron connected to every neuron in the next layer)\n",
    "model.add(Dense(8, activation='tanh'))\n",
    "# Last layer simple sigmoid function to output 0 or 1 (our label)\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit (Train) the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "29/29 - 4s - loss: 0.6388 - accuracy: 0.5495 - 4s/epoch - 152ms/step\n",
      "Epoch 2/50\n",
      "29/29 - 0s - loss: 0.6243 - accuracy: 0.5724 - 130ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "29/29 - 0s - loss: 0.6117 - accuracy: 0.6823 - 70ms/epoch - 2ms/step\n",
      "Epoch 4/50\n",
      "29/29 - 0s - loss: 0.5978 - accuracy: 0.7410 - 62ms/epoch - 2ms/step\n",
      "Epoch 5/50\n",
      "29/29 - 0s - loss: 0.5837 - accuracy: 0.7856 - 83ms/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "29/29 - 0s - loss: 0.5683 - accuracy: 0.8041 - 70ms/epoch - 2ms/step\n",
      "Epoch 7/50\n",
      "29/29 - 0s - loss: 0.5520 - accuracy: 0.8161 - 62ms/epoch - 2ms/step\n",
      "Epoch 8/50\n",
      "29/29 - 0s - loss: 0.5343 - accuracy: 0.8183 - 61ms/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "29/29 - 0s - loss: 0.5157 - accuracy: 0.8215 - 69ms/epoch - 2ms/step\n",
      "Epoch 10/50\n",
      "29/29 - 0s - loss: 0.4952 - accuracy: 0.8400 - 66ms/epoch - 2ms/step\n",
      "Epoch 11/50\n",
      "29/29 - 0s - loss: 0.4748 - accuracy: 0.8444 - 62ms/epoch - 2ms/step\n",
      "Epoch 12/50\n",
      "29/29 - 0s - loss: 0.4520 - accuracy: 0.8662 - 55ms/epoch - 2ms/step\n",
      "Epoch 13/50\n",
      "29/29 - 0s - loss: 0.4242 - accuracy: 0.8662 - 56ms/epoch - 2ms/step\n",
      "Epoch 14/50\n",
      "29/29 - 0s - loss: 0.3958 - accuracy: 0.8629 - 63ms/epoch - 2ms/step\n",
      "Epoch 15/50\n",
      "29/29 - 0s - loss: 0.3717 - accuracy: 0.8694 - 56ms/epoch - 2ms/step\n",
      "Epoch 16/50\n",
      "29/29 - 0s - loss: 0.3487 - accuracy: 0.8749 - 65ms/epoch - 2ms/step\n",
      "Epoch 17/50\n",
      "29/29 - 0s - loss: 0.3274 - accuracy: 0.8781 - 58ms/epoch - 2ms/step\n",
      "Epoch 18/50\n",
      "29/29 - 0s - loss: 0.3073 - accuracy: 0.8912 - 66ms/epoch - 2ms/step\n",
      "Epoch 19/50\n",
      "29/29 - 0s - loss: 0.2885 - accuracy: 0.8912 - 69ms/epoch - 2ms/step\n",
      "Epoch 20/50\n",
      "29/29 - 0s - loss: 0.2710 - accuracy: 0.9032 - 66ms/epoch - 2ms/step\n",
      "Epoch 21/50\n",
      "29/29 - 0s - loss: 0.2551 - accuracy: 0.9097 - 61ms/epoch - 2ms/step\n",
      "Epoch 22/50\n",
      "29/29 - 0s - loss: 0.2395 - accuracy: 0.9227 - 56ms/epoch - 2ms/step\n",
      "Epoch 23/50\n",
      "29/29 - 0s - loss: 0.2264 - accuracy: 0.9249 - 65ms/epoch - 2ms/step\n",
      "Epoch 24/50\n",
      "29/29 - 0s - loss: 0.2135 - accuracy: 0.9282 - 56ms/epoch - 2ms/step\n",
      "Epoch 25/50\n",
      "29/29 - 0s - loss: 0.2015 - accuracy: 0.9380 - 55ms/epoch - 2ms/step\n",
      "Epoch 26/50\n",
      "29/29 - 0s - loss: 0.1901 - accuracy: 0.9423 - 65ms/epoch - 2ms/step\n",
      "Epoch 27/50\n",
      "29/29 - 0s - loss: 0.1795 - accuracy: 0.9467 - 55ms/epoch - 2ms/step\n",
      "Epoch 28/50\n",
      "29/29 - 0s - loss: 0.1702 - accuracy: 0.9521 - 60ms/epoch - 2ms/step\n",
      "Epoch 29/50\n",
      "29/29 - 0s - loss: 0.1614 - accuracy: 0.9532 - 55ms/epoch - 2ms/step\n",
      "Epoch 30/50\n",
      "29/29 - 0s - loss: 0.1532 - accuracy: 0.9532 - 67ms/epoch - 2ms/step\n",
      "Epoch 31/50\n",
      "29/29 - 0s - loss: 0.1454 - accuracy: 0.9576 - 56ms/epoch - 2ms/step\n",
      "Epoch 32/50\n",
      "29/29 - 0s - loss: 0.1385 - accuracy: 0.9608 - 54ms/epoch - 2ms/step\n",
      "Epoch 33/50\n",
      "29/29 - 0s - loss: 0.1322 - accuracy: 0.9619 - 64ms/epoch - 2ms/step\n",
      "Epoch 34/50\n",
      "29/29 - 0s - loss: 0.1255 - accuracy: 0.9652 - 55ms/epoch - 2ms/step\n",
      "Epoch 35/50\n",
      "29/29 - 0s - loss: 0.1204 - accuracy: 0.9619 - 55ms/epoch - 2ms/step\n",
      "Epoch 36/50\n",
      "29/29 - 0s - loss: 0.1142 - accuracy: 0.9652 - 68ms/epoch - 2ms/step\n",
      "Epoch 37/50\n",
      "29/29 - 0s - loss: 0.1089 - accuracy: 0.9663 - 64ms/epoch - 2ms/step\n",
      "Epoch 38/50\n",
      "29/29 - 0s - loss: 0.1040 - accuracy: 0.9706 - 57ms/epoch - 2ms/step\n",
      "Epoch 39/50\n",
      "29/29 - 0s - loss: 0.1000 - accuracy: 0.9706 - 57ms/epoch - 2ms/step\n",
      "Epoch 40/50\n",
      "29/29 - 0s - loss: 0.0949 - accuracy: 0.9782 - 63ms/epoch - 2ms/step\n",
      "Epoch 41/50\n",
      "29/29 - 0s - loss: 0.0910 - accuracy: 0.9771 - 55ms/epoch - 2ms/step\n",
      "Epoch 42/50\n",
      "29/29 - 0s - loss: 0.0874 - accuracy: 0.9782 - 58ms/epoch - 2ms/step\n",
      "Epoch 43/50\n",
      "29/29 - 0s - loss: 0.0837 - accuracy: 0.9728 - 66ms/epoch - 2ms/step\n",
      "Epoch 44/50\n",
      "29/29 - 0s - loss: 0.0806 - accuracy: 0.9782 - 65ms/epoch - 2ms/step\n",
      "Epoch 45/50\n",
      "29/29 - 0s - loss: 0.0774 - accuracy: 0.9804 - 74ms/epoch - 3ms/step\n",
      "Epoch 46/50\n",
      "29/29 - 0s - loss: 0.0741 - accuracy: 0.9793 - 68ms/epoch - 2ms/step\n",
      "Epoch 47/50\n",
      "29/29 - 0s - loss: 0.0713 - accuracy: 0.9859 - 71ms/epoch - 2ms/step\n",
      "Epoch 48/50\n",
      "29/29 - 0s - loss: 0.0688 - accuracy: 0.9859 - 69ms/epoch - 2ms/step\n",
      "Epoch 49/50\n",
      "29/29 - 0s - loss: 0.0661 - accuracy: 0.9880 - 57ms/epoch - 2ms/step\n",
      "Epoch 50/50\n",
      "29/29 - 0s - loss: 0.0639 - accuracy: 0.9891 - 54ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20ba4db4890>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play around with number of epochs as well!\n",
    "model.fit(scaled_X_train,y_train,epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting New Unseen Data\n",
    "\n",
    "Let's see how we did by predicting on **new data**. Remember, our model has **never** seen the test data that we scaled previously! This process is the exact same process you would use on totally brand new data. For example , a brand new bank note that you just analyzed ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62098955, 0.75284662, 0.21900753, 0.5730998 ],\n",
       "       [0.48778602, 0.82175665, 0.09174727, 0.56211079],\n",
       "       [0.51158363, 0.77924916, 0.13830875, 0.50392598],\n",
       "       ...,\n",
       "       [0.76115065, 0.62415668, 0.27251204, 0.83616757],\n",
       "       [0.9047516 , 0.90329171, 0.03658247, 0.42267079],\n",
       "       [0.49296526, 0.86039507, 0.06714046, 0.1622583 ]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spits out probabilities by default.\n",
    "# model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.73516572e-02],\n",
       "       [5.62993765e-01],\n",
       "       [2.49164253e-01],\n",
       "       [6.74637314e-03],\n",
       "       [3.78713794e-02],\n",
       "       [4.12620325e-03],\n",
       "       [4.83450405e-02],\n",
       "       [6.19526161e-03],\n",
       "       [9.25675500e-03],\n",
       "       [1.89561825e-02],\n",
       "       [7.73932636e-01],\n",
       "       [9.81675208e-01],\n",
       "       [8.65958072e-03],\n",
       "       [9.61376190e-01],\n",
       "       [1.17634773e-01],\n",
       "       [5.74482143e-01],\n",
       "       [9.76874530e-01],\n",
       "       [9.81388032e-01],\n",
       "       [9.91940320e-01],\n",
       "       [8.76533210e-01],\n",
       "       [1.83277167e-02],\n",
       "       [7.19408039e-03],\n",
       "       [9.55202818e-01],\n",
       "       [1.47847217e-02],\n",
       "       [9.98501062e-01],\n",
       "       [1.20442379e-02],\n",
       "       [3.53114493e-03],\n",
       "       [9.75972295e-01],\n",
       "       [1.10887329e-03],\n",
       "       [2.47173291e-03],\n",
       "       [9.22099948e-01],\n",
       "       [1.31103164e-02],\n",
       "       [1.83415506e-02],\n",
       "       [9.97533619e-01],\n",
       "       [9.95791316e-01],\n",
       "       [5.96195646e-03],\n",
       "       [7.32089639e-01],\n",
       "       [9.63984907e-01],\n",
       "       [9.82844889e-01],\n",
       "       [1.18803335e-02],\n",
       "       [3.03294929e-03],\n",
       "       [9.10647631e-01],\n",
       "       [9.57717836e-01],\n",
       "       [2.35602986e-02],\n",
       "       [9.97334003e-01],\n",
       "       [9.82312083e-01],\n",
       "       [9.93624628e-01],\n",
       "       [5.47426194e-03],\n",
       "       [7.96556193e-03],\n",
       "       [6.48160279e-01],\n",
       "       [7.98076298e-03],\n",
       "       [5.38518047e-03],\n",
       "       [2.12315042e-02],\n",
       "       [9.05507989e-03],\n",
       "       [1.09110616e-01],\n",
       "       [9.37554657e-01],\n",
       "       [2.55584512e-02],\n",
       "       [4.91110468e-03],\n",
       "       [3.43240332e-03],\n",
       "       [9.33109503e-03],\n",
       "       [9.93672132e-01],\n",
       "       [9.28811450e-03],\n",
       "       [6.08634114e-01],\n",
       "       [2.37787026e-03],\n",
       "       [1.25708366e-02],\n",
       "       [5.95437316e-03],\n",
       "       [7.01777777e-03],\n",
       "       [3.31094787e-02],\n",
       "       [9.12145302e-02],\n",
       "       [5.97882926e-01],\n",
       "       [9.55369115e-01],\n",
       "       [1.89218670e-02],\n",
       "       [9.60781395e-01],\n",
       "       [2.75528012e-03],\n",
       "       [9.43355978e-01],\n",
       "       [1.79094940e-01],\n",
       "       [6.95111183e-03],\n",
       "       [9.82055187e-01],\n",
       "       [9.85705197e-01],\n",
       "       [8.57781529e-01],\n",
       "       [7.68667340e-01],\n",
       "       [7.66703337e-02],\n",
       "       [7.42518425e-01],\n",
       "       [9.67978686e-03],\n",
       "       [1.62131209e-02],\n",
       "       [2.47473759e-03],\n",
       "       [1.72368549e-02],\n",
       "       [9.97513294e-01],\n",
       "       [4.10167038e-01],\n",
       "       [2.24731490e-02],\n",
       "       [1.65034141e-02],\n",
       "       [6.16244692e-03],\n",
       "       [8.78368914e-01],\n",
       "       [9.65297580e-01],\n",
       "       [1.05547765e-02],\n",
       "       [9.90244985e-01],\n",
       "       [9.98236239e-01],\n",
       "       [1.29616916e-01],\n",
       "       [1.85776111e-02],\n",
       "       [6.15941510e-02],\n",
       "       [9.89300668e-01],\n",
       "       [4.59767925e-03],\n",
       "       [5.18572479e-02],\n",
       "       [1.65943597e-02],\n",
       "       [9.97335374e-01],\n",
       "       [5.29633043e-03],\n",
       "       [1.11871641e-02],\n",
       "       [6.90703511e-01],\n",
       "       [9.93758559e-01],\n",
       "       [9.64622736e-01],\n",
       "       [9.76169229e-01],\n",
       "       [9.14606035e-01],\n",
       "       [3.80710862e-03],\n",
       "       [9.95914280e-01],\n",
       "       [9.41509902e-01],\n",
       "       [9.93795872e-01],\n",
       "       [3.18251387e-03],\n",
       "       [9.97164309e-01],\n",
       "       [9.91476476e-01],\n",
       "       [7.01617217e-03],\n",
       "       [9.55972493e-01],\n",
       "       [1.59069356e-02],\n",
       "       [8.79416645e-01],\n",
       "       [5.56310918e-03],\n",
       "       [3.38545144e-01],\n",
       "       [3.14536840e-02],\n",
       "       [9.79610622e-01],\n",
       "       [8.71671319e-01],\n",
       "       [2.46285349e-02],\n",
       "       [9.75844920e-01],\n",
       "       [9.95455742e-01],\n",
       "       [8.69022403e-03],\n",
       "       [2.09018607e-02],\n",
       "       [7.13150017e-03],\n",
       "       [3.44662322e-03],\n",
       "       [8.69247224e-03],\n",
       "       [9.54209805e-01],\n",
       "       [2.27505318e-03],\n",
       "       [7.69996345e-02],\n",
       "       [8.43095362e-01],\n",
       "       [3.60401832e-02],\n",
       "       [6.50304975e-03],\n",
       "       [9.00019705e-01],\n",
       "       [3.44056194e-03],\n",
       "       [8.43387604e-01],\n",
       "       [3.63094002e-01],\n",
       "       [5.23567200e-01],\n",
       "       [7.57454634e-01],\n",
       "       [8.40129852e-01],\n",
       "       [1.95098557e-02],\n",
       "       [7.37772524e-01],\n",
       "       [9.82209742e-01],\n",
       "       [9.88915563e-01],\n",
       "       [1.74118998e-03],\n",
       "       [9.34988201e-01],\n",
       "       [3.54429847e-03],\n",
       "       [9.54057813e-01],\n",
       "       [4.44178693e-02],\n",
       "       [2.78511196e-02],\n",
       "       [7.88862072e-03],\n",
       "       [9.97855961e-01],\n",
       "       [9.81443584e-01],\n",
       "       [9.95044768e-01],\n",
       "       [9.94579375e-01],\n",
       "       [9.51399386e-01],\n",
       "       [1.15041742e-02],\n",
       "       [9.79672492e-01],\n",
       "       [6.80388650e-03],\n",
       "       [4.82458808e-03],\n",
       "       [6.36687642e-03],\n",
       "       [2.44041860e-01],\n",
       "       [5.02981246e-03],\n",
       "       [2.21832357e-02],\n",
       "       [9.94941592e-01],\n",
       "       [3.30275670e-03],\n",
       "       [4.13283380e-03],\n",
       "       [9.88473415e-01],\n",
       "       [9.98185456e-01],\n",
       "       [7.84344599e-02],\n",
       "       [6.18271250e-03],\n",
       "       [1.35674506e-01],\n",
       "       [1.11449957e-02],\n",
       "       [9.91116762e-01],\n",
       "       [1.72154643e-02],\n",
       "       [9.91075933e-01],\n",
       "       [1.63391251e-02],\n",
       "       [9.68728125e-01],\n",
       "       [9.79395926e-01],\n",
       "       [6.25699805e-03],\n",
       "       [2.13708729e-02],\n",
       "       [9.82503533e-01],\n",
       "       [1.04183722e-02],\n",
       "       [9.30823609e-02],\n",
       "       [9.45589900e-01],\n",
       "       [9.65780139e-01],\n",
       "       [9.85112906e-01],\n",
       "       [9.48034883e-01],\n",
       "       [5.76306619e-02],\n",
       "       [1.34938240e-01],\n",
       "       [9.91084635e-01],\n",
       "       [9.98179793e-01],\n",
       "       [9.91498649e-01],\n",
       "       [1.09847775e-02],\n",
       "       [1.03211170e-02],\n",
       "       [9.92251277e-01],\n",
       "       [9.39624965e-01],\n",
       "       [9.53194201e-01],\n",
       "       [8.58485222e-01],\n",
       "       [4.29576896e-02],\n",
       "       [9.73840337e-03],\n",
       "       [3.03025246e-01],\n",
       "       [4.82967356e-03],\n",
       "       [1.50727211e-02],\n",
       "       [7.40123391e-02],\n",
       "       [6.09720638e-03],\n",
       "       [6.69099391e-03],\n",
       "       [1.79094940e-01],\n",
       "       [1.64387599e-02],\n",
       "       [9.96055245e-01],\n",
       "       [9.91512179e-01],\n",
       "       [9.81359184e-01],\n",
       "       [9.42419827e-01],\n",
       "       [9.84245598e-01],\n",
       "       [6.31037494e-03],\n",
       "       [9.49786901e-01],\n",
       "       [2.19235639e-03],\n",
       "       [1.84323899e-02],\n",
       "       [9.91907060e-01],\n",
       "       [9.91423130e-01],\n",
       "       [8.99987876e-01],\n",
       "       [9.87102926e-01],\n",
       "       [1.11937532e-02],\n",
       "       [8.80318224e-01],\n",
       "       [4.02529910e-03],\n",
       "       [9.66673315e-01],\n",
       "       [9.96744096e-01],\n",
       "       [9.64093804e-01],\n",
       "       [9.91987169e-01],\n",
       "       [2.85759829e-02],\n",
       "       [4.61815670e-03],\n",
       "       [1.52366519e-01],\n",
       "       [9.58635747e-01],\n",
       "       [6.49779616e-03],\n",
       "       [9.93561506e-01],\n",
       "       [9.72315252e-01],\n",
       "       [9.89517570e-01],\n",
       "       [1.35748573e-02],\n",
       "       [2.88956501e-02],\n",
       "       [8.97679199e-03],\n",
       "       [8.88763368e-03],\n",
       "       [1.98152475e-03],\n",
       "       [1.53728174e-02],\n",
       "       [9.89791870e-01],\n",
       "       [3.01221609e-01],\n",
       "       [2.84523517e-01],\n",
       "       [7.07866764e-03],\n",
       "       [1.27893751e-02],\n",
       "       [2.67634750e-03],\n",
       "       [9.60410714e-01],\n",
       "       [9.89096463e-01],\n",
       "       [4.99511743e-03],\n",
       "       [9.53145176e-02],\n",
       "       [8.13495554e-03],\n",
       "       [9.85874772e-01],\n",
       "       [9.16775346e-01],\n",
       "       [1.68627705e-02],\n",
       "       [9.76130962e-01],\n",
       "       [1.24554345e-02],\n",
       "       [9.98018682e-01],\n",
       "       [9.73061919e-01],\n",
       "       [9.87776458e-01],\n",
       "       [9.10250962e-01],\n",
       "       [7.58509757e-03],\n",
       "       [5.62986851e-01],\n",
       "       [6.17364235e-03],\n",
       "       [7.14877713e-03],\n",
       "       [1.97297595e-02],\n",
       "       [9.83148158e-01],\n",
       "       [5.91284549e-03],\n",
       "       [3.24522890e-02],\n",
       "       [9.95769799e-01],\n",
       "       [3.25745437e-03],\n",
       "       [9.93618131e-01],\n",
       "       [5.35459630e-03],\n",
       "       [1.18305450e-02],\n",
       "       [9.87965167e-01],\n",
       "       [5.39759221e-03],\n",
       "       [4.67361743e-03],\n",
       "       [4.18315781e-03],\n",
       "       [2.77323164e-02],\n",
       "       [8.92686564e-03],\n",
       "       [9.98070419e-01],\n",
       "       [9.98037040e-01],\n",
       "       [5.28677786e-03],\n",
       "       [9.95365977e-01],\n",
       "       [1.50674535e-02],\n",
       "       [9.33326840e-01],\n",
       "       [9.97742176e-01],\n",
       "       [2.68210913e-03],\n",
       "       [3.03016156e-01],\n",
       "       [2.38863844e-02],\n",
       "       [6.57530641e-03],\n",
       "       [2.60635279e-03],\n",
       "       [3.10674068e-02],\n",
       "       [6.72170008e-03],\n",
       "       [9.84122515e-01],\n",
       "       [9.95700181e-01],\n",
       "       [9.88423049e-01],\n",
       "       [1.94656197e-02],\n",
       "       [2.54829321e-03],\n",
       "       [8.84629071e-01],\n",
       "       [9.57347393e-01],\n",
       "       [5.23231039e-03],\n",
       "       [1.20460697e-01],\n",
       "       [1.02231726e-02],\n",
       "       [3.00644781e-03],\n",
       "       [1.01767685e-02],\n",
       "       [5.27471770e-03],\n",
       "       [9.93342638e-01],\n",
       "       [2.90267672e-02],\n",
       "       [9.89205837e-01],\n",
       "       [8.36809576e-01],\n",
       "       [9.84845102e-01],\n",
       "       [1.78593732e-02],\n",
       "       [3.29012587e-03],\n",
       "       [4.39629611e-03],\n",
       "       [1.57550839e-03],\n",
       "       [9.21646714e-01],\n",
       "       [1.20598145e-01],\n",
       "       [1.81622396e-03],\n",
       "       [8.46583210e-03],\n",
       "       [1.08326385e-02],\n",
       "       [2.03020088e-02],\n",
       "       [6.65506581e-03],\n",
       "       [2.20634378e-02],\n",
       "       [9.94544089e-01],\n",
       "       [2.07775906e-02],\n",
       "       [9.57424402e-01],\n",
       "       [9.95576799e-01],\n",
       "       [8.87491107e-01],\n",
       "       [9.70226228e-01],\n",
       "       [1.15179764e-02],\n",
       "       [9.37342823e-01],\n",
       "       [9.93445992e-01],\n",
       "       [2.67279125e-03],\n",
       "       [9.73158240e-01],\n",
       "       [9.28345442e-01],\n",
       "       [4.72196005e-03],\n",
       "       [3.23993973e-02],\n",
       "       [8.56168747e-01],\n",
       "       [2.60531083e-02],\n",
       "       [2.71875923e-03],\n",
       "       [9.22631323e-01],\n",
       "       [9.63624101e-03],\n",
       "       [9.67147171e-01],\n",
       "       [5.50119113e-03],\n",
       "       [8.67483735e-01],\n",
       "       [9.67926919e-01],\n",
       "       [4.79547633e-03],\n",
       "       [8.35122447e-03],\n",
       "       [9.86015558e-01],\n",
       "       [6.59870869e-03],\n",
       "       [3.06617469e-02],\n",
       "       [3.08922250e-02],\n",
       "       [1.87120773e-03],\n",
       "       [3.11110192e-03],\n",
       "       [8.58088396e-03],\n",
       "       [9.92923141e-01],\n",
       "       [2.99070645e-02],\n",
       "       [1.32467784e-02],\n",
       "       [9.95174825e-01],\n",
       "       [1.60802156e-01],\n",
       "       [1.40167605e-02],\n",
       "       [2.82178484e-02],\n",
       "       [1.50522413e-02],\n",
       "       [9.86039162e-01],\n",
       "       [9.89453673e-01],\n",
       "       [4.09845542e-03],\n",
       "       [9.84506726e-01],\n",
       "       [5.74850431e-03],\n",
       "       [9.95752156e-01],\n",
       "       [9.35297191e-01],\n",
       "       [9.36887741e-01],\n",
       "       [7.99963810e-03],\n",
       "       [9.24236357e-01],\n",
       "       [4.94290233e-01],\n",
       "       [6.44797692e-03],\n",
       "       [9.92711842e-01],\n",
       "       [9.83254075e-01],\n",
       "       [2.68681999e-03],\n",
       "       [9.93479371e-01],\n",
       "       [9.64321569e-03],\n",
       "       [1.52949886e-02],\n",
       "       [1.17971897e-02],\n",
       "       [9.95628834e-01],\n",
       "       [9.86790895e-01],\n",
       "       [7.66810179e-01],\n",
       "       [7.28880474e-03],\n",
       "       [4.96103577e-02],\n",
       "       [3.59936617e-03],\n",
       "       [1.13492636e-02],\n",
       "       [9.86524820e-01],\n",
       "       [1.20308204e-02],\n",
       "       [4.92385868e-03],\n",
       "       [9.84265625e-01],\n",
       "       [9.90420580e-01],\n",
       "       [9.13672030e-01],\n",
       "       [9.85897303e-01],\n",
       "       [6.98819291e-03],\n",
       "       [9.75686908e-01],\n",
       "       [9.04274080e-03],\n",
       "       [3.43421698e-02],\n",
       "       [9.42855060e-01],\n",
       "       [7.70747602e-01],\n",
       "       [6.17672391e-02],\n",
       "       [9.95973229e-01],\n",
       "       [3.08696628e-02],\n",
       "       [9.51285839e-01],\n",
       "       [1.09883202e-02],\n",
       "       [1.05246706e-02],\n",
       "       [3.53089324e-03],\n",
       "       [9.91845310e-01],\n",
       "       [8.85751903e-01],\n",
       "       [5.40804630e-03],\n",
       "       [6.23001298e-03],\n",
       "       [5.38355391e-03],\n",
       "       [9.94561970e-01],\n",
       "       [7.16094766e-03],\n",
       "       [9.78208799e-03],\n",
       "       [9.53834951e-01],\n",
       "       [5.24276961e-03],\n",
       "       [4.44630394e-03],\n",
       "       [7.89422821e-03],\n",
       "       [5.79908071e-03],\n",
       "       [9.05009925e-01],\n",
       "       [5.98119758e-03],\n",
       "       [9.87762749e-01],\n",
       "       [9.05520283e-04],\n",
       "       [3.03016156e-01],\n",
       "       [9.73772109e-01],\n",
       "       [1.16102854e-02],\n",
       "       [1.29912747e-02],\n",
       "       [1.10879131e-02],\n",
       "       [1.04331039e-03],\n",
       "       [1.12481369e-03],\n",
       "       [1.17360475e-02],\n",
       "       [9.77751911e-01],\n",
       "       [9.27714705e-01],\n",
       "       [7.29271863e-03],\n",
       "       [5.71376039e-03],\n",
       "       [6.46289950e-03],\n",
       "       [1.08308520e-03],\n",
       "       [1.35674506e-01]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance\n",
    "\n",
    "So how well did we do? How do we actually measure \"well\". Is 95% accuracy good enough? It all depends on the situation. Also we need to take into account things like recall and precision. Make sure to watch the video discussion on classification evaluation before running this code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.060606613755226135, 0.9823399782180786]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=scaled_X_test,y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = (model.predict(scaled_X_test) > 0.5).astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[254,   3],\n",
       "       [  5, 191]], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       257\n",
      "         1.0       0.98      0.97      0.98       196\n",
      "\n",
      "    accuracy                           0.98       453\n",
      "   macro avg       0.98      0.98      0.98       453\n",
      "weighted avg       0.98      0.98      0.98       453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "\n",
    "Now that we have a model trained, let's see how we can save and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = load_model('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You now know how to preprocess data, train a neural network, and evaluate its classification performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
